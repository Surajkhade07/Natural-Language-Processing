!pip install nltk


paragraph='''
Rohit Gurunath Sharma (born 30 April 1987) is an Indian international cricketer and the former captain of the Indian national cricket team. He is a right-handed batsman who plays for Mumbai Indians in Indian Premier League and for Mumbai in domestic cricket. In the year 2024 and 2025 he announced his retirement from T20Is and Test Cricket.[3][4] He was also a member of the teams that won the 2007 T20 World Cup, the 2013 ICC Champions Trophy and was the winning captain of the 2024 Men's T20 World Cup and 2025 ICC Champions Trophy.

Sharma holds several batting records which include most runs in T20 Internationals, most sixes in international cricket,[a] most double centuries in ODI cricket (3), most centuries at Cricket World Cups (7) and joint most hundreds in Twenty20 Internationals (5)[6].He also holds the world record for the highest individual score (264) in a One Day International (ODI) and also holds the record for scoring most hundreds (five) in a single Cricket World Cup, for which he won the ICC Men's ODI Cricketer of the Year award in 2019.[7] He is the first and only captain to lead a team in all[b] ICC tournament finals.[8]
'''


paragraph


import nltk
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords


## tokenization paragraph-sentence-words
nltk.download('punkt_tab')
sentence=nltk.sent_tokenize(paragraph)


print(sentence)


type(sentence)


stemmer=PorterStemmer()


stemmer.stem('going')


stemmer.stem('history')


from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')


lemmatizer=WordNetLemmatizer()


lemmatizer.lemmatize('history')


lemmatizer.lemmatize(paragraph)


import re


# to clean the sentence removing special characters
corpus=[]
nltk.download('stopwords')
for i in range (len(sentence)):
    clean=re.sub('[^a-zA-Z]',' ',sentence[i])
    clean=clean.lower()
    clean=clean.split()
    clean=[lemmatizer.lemmatize(word) for word in clean if not word in set(stopwords.words('english'))]
    clean=' '.join(clean)
    corpus.append(clean)


print(corpus)


##stemming
nltk.download('stopwords')
for i in corpus:
    words = nltk.word_tokenize(i)
    for word in words:
        if word not in set(stopwords.words('english')):
            print(lemmatizer.lemmatize(word))
            



##use of bag of words
from sklearn.feature_extraction.text import CountVectorizer
cv=CountVectorizer(binary=True,ngram_range=(2,3))


x=cv.fit_transform(corpus)


cv.vocabulary_


corpus[0]


x[0].toarray()


### Use of TFIDF
from sklearn.feature_extraction.text import TfidfVectorizer


tf=TfidfVectorizer(ngram_range=(3,3)
)


x=tf.fit_transform(corpus)


tf.vocabulary_



corpus[0]


x[0].toarray()



s
